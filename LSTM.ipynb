{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layers.utils.LSTMCell import LSTMCell\n",
    "from model.Model import Model\n",
    "from utils.model_loss import cross_entropy_loss_npdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_cell():\n",
    "    np.random.seed(0)\n",
    "    weights = {'Wc_i': np.random.rand(3, 5),\n",
    "               'Wu_i': np.random.rand(3, 5),\n",
    "               'Wf_i': np.random.rand(3, 5),\n",
    "               'Wo_i': np.random.rand(3, 5),\n",
    "               'Wc_h': np.random.rand(5, 5),\n",
    "               'Wu_h': np.random.rand(5, 5),\n",
    "               'Wf_h': np.random.rand(5, 5),\n",
    "               'Wo_h': np.random.rand(5, 5)}\n",
    "    biases = {'bc': np.zeros(5),\n",
    "              'bu': np.zeros(5),\n",
    "              'bf': np.zeros(5),\n",
    "              'bo': np.zeros(5)}\n",
    "    np.random.seed(0)\n",
    "    return LSTMCell(3, 5, weights, biases)\n",
    "\n",
    "def create_toy_data():\n",
    "    np.random.seed(1)\n",
    "    return np.random.uniform(-1, 1, (5, 3)), np.random.uniform(-1, 1, (5, 5)), np.random.uniform(-1, 1, (5, 5)), np.array([0, 1, 1, 4, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = create_toy_cell()\n",
    "x, h, c, y = create_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_h_next = np.array([[ 0.05432073, -0.0240766, -0.08374854, 0.00076676, -0.14279187],\n",
    "                           [-0.26776613, 0.31730641, -0.21700223, -0.28390231, -0.26996079],\n",
    "                           [-0.13447024, -0.08660263, -0.10022437, -0.03162882, -0.05513277],\n",
    "                           [-0.07544577, 0.17997456, -0.08875356, 0.28115015, 0.28349453],\n",
    "                           [-0.20265419, -0.04393417, 0.15789945, -0.00593344, -0.18382524]])\n",
    "\n",
    "correct_c_next = np.array([[ 0.14534732, -0.12841532, -0.17270371, 0.0028584, -0.33097237],\n",
    "                           [-0.40553132, 0.52289938, -0.46666314, -0.41915372, -0.52302227],\n",
    "                           [-0.5563115, -0.20751267, -0.33762134, -0.14975814, -0.34766355],\n",
    "                           [-0.12754703, 0.35234149, -0.1480342, 0.517628, 0.47883466],\n",
    "                           [-0.30773508, -0.14781391, 0.21382212, -0.01451843, -0.31231354]])\n",
    "\n",
    "c_next, h_next, h_up  = cell.forward_npdl(x, h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Your h_next:')\n",
    "print(h_next)\n",
    "print()\n",
    "print('correct h_next:')\n",
    "print(correct_h_next)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your h_next and correct h_next: ', np.sum(np.abs(h_next - correct_h_next)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Your c_next:')\n",
    "print(c_next)\n",
    "print()\n",
    "print('correct c_next:')\n",
    "print(correct_c_next)\n",
    "print()\n",
    "\n",
    "# La différence devrait être assez basse, en principe inférieure à 1e-7.\n",
    "print('Difference between your c_next and correct c_next: ', np.sum(np.abs(c_next - correct_c_next)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.Dense import Dense\n",
    "from layers.TimeDistributed import TimeDistributed\n",
    "from utils.model_loss import td_cross_entropy_loss_npdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_timedist():\n",
    "    np.random.seed(0)\n",
    "    return TimeDistributed(Dense(dim_input=5, dim_output=2), out_size=2)\n",
    "\n",
    "def create_toy_sequence():\n",
    "    np.random.seed(1)\n",
    "    return np.random.uniform(-1, 1, (3, 4, 5)), np.array([[1, 1, 1, 0],\n",
    "                                                          [1, 1, 0, 1],\n",
    "                                                          [0, 1, 1, 1]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = create_toy_timedist()\n",
    "x, y = create_toy_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = layer.forward_npdl(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dScores, softmax_output = td_cross_entropy_loss_npdl(z, y, 0.0, {'l1': layer.get_params()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dX = layer.backward_npdl(dScores)\n",
    "dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Stanford Sentiment Treebank V1.0\n",
    "dictionary = pd.read_csv('datasets/stanfordSentimentTreebank/dictionary.txt', header=None, sep='|')\n",
    "dictionary = dictionary.rename(columns={0:'phrase', 1:'phrase_id'})\n",
    "\n",
    "dataset_split = pd.read_csv('datasets/stanfordSentimentTreebank/datasetSplit.txt', sep=',')\n",
    "\n",
    "dataset_sentences = pd.read_csv('datasets/stanfordSentimentTreebank/datasetSentences.txt', sep='\\t')\n",
    "\n",
    "dataset_labels = pd.read_csv('datasets/stanfordSentimentTreebank/sentiment_labels.txt', sep='|')\n",
    "dataset_labels = dataset_labels.rename(columns={'phrase ids':'phrase_id', 'sentiment values':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les phrase_id des sentence\n",
    "sentences_merged = dataset_sentences.merge(dictionary, left_on='sentence', right_on='phrase', how='left').drop(columns=['phrase'])\n",
    "\n",
    "# Retirer les sentence qui n'ont pas de phrase_id\n",
    "sentences_clean = sentences_merged[~sentences_merged.phrase_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenir les labels\n",
    "sentences_with_labels = sentences_clean.merge(dataset_labels, on='phrase_id', how='left').drop(columns=['phrase_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separation train - valid - test\n",
    "sentences_split = sentences_with_labels.merge(dataset_split, on='sentence_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_en = BPEmb(lang=\"en\", dim=25, vs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_embed(value, embedder, max_length):\n",
    "    emb = embedder.embed(value)\n",
    "    return np.pad(emb, ((0, max_length - emb.shape[0]), (0, 0)), 'constant', constant_values=(0))\n",
    "\n",
    "def get_longest(value, embedder):\n",
    "    emb = embedder.embed(value)\n",
    "    return emb.shape[0]\n",
    "\n",
    "def convert_sentiment(value):\n",
    "    if value <= 0.4:\n",
    "        return 0\n",
    "    if value <= 0.6:\n",
    "        return 1\n",
    "    return 2\n",
    "    \n",
    "\n",
    "sentences_split['len'] = sentences_split.apply(lambda x: get_longest(x['sentence'], bpemb_en), axis=1)\n",
    "\n",
    "max_len = sentences_split.len.max()\n",
    "print(max_len)\n",
    "\n",
    "sentences_split['embedding'] = sentences_split.apply(lambda x: call_embed(x['sentence'], bpemb_en, max_len), axis=1)\n",
    "sentences_split['sentiment_label'] = sentences_split.apply(lambda x: convert_sentiment(x['sentiment']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sentences_split[sentences_split.splitset_label == 1].drop(columns=['splitset_label'])\n",
    "valid = sentences_split[sentences_split.splitset_label == 2].drop(columns=['splitset_label'])\n",
    "test = sentences_split[sentences_split.splitset_label == 3].drop(columns=['splitset_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train.embedding.tolist())\n",
    "valid_data = np.array(valid.embedding.tolist())\n",
    "test_data = np.array(test.embedding.tolist())\n",
    "\n",
    "train_labels = np.array(train.sentiment_label.tolist())\n",
    "valid_labels = np.array(valid.sentiment_label.tolist())\n",
    "test_labels = np.array(test.sentiment_label.tolist())\n",
    "\n",
    "train_data = np.concatenate((train_data, test_data), axis=0)\n",
    "train_labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Model import Model\n",
    "from layers.LSTM import LSTM\n",
    "from layers.Dense import Dense\n",
    "from layers.Flatten import Flatten\n",
    "from utils.model_loss import cross_entropy_loss_npdl\n",
    "\n",
    "def create_lstm_network():\n",
    "    model = Model()\n",
    "    \n",
    "    lstm1 = LSTM(58, 25, 50, weight_scale=None)\n",
    "    dense1 = Dense(50, 3, weight_scale=None)\n",
    "    \n",
    "    flatten = Flatten()\n",
    "    \n",
    "    model.add(lstm1)\n",
    "    model.add(flatten)\n",
    "    model.add(dense1)\n",
    "    model.add_loss(cross_entropy_loss_npdl)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_lstm_network()\n",
    "predictions = model.predict(test_data[:32])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.forward_npdl(train_data[:32])\n",
    "loss, dScores, softmax_output = model.calculate_loss(scores, train_labels[:32], 0.0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Solver import epoch_solver_npdl, Adam, SGD\n",
    "\n",
    "model = create_lstm_network()\n",
    "\n",
    "optimizer = Adam(1e-3, model)\n",
    "    \n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver_npdl(train_data, \n",
    "                                                                          train_labels,\n",
    "                                                                          valid_data,\n",
    "                                                                          valid_labels,\n",
    "                                                                          2e-4,\n",
    "                                                                          optimizer,\n",
    "                                                                          lr_decay=0.95,\n",
    "                                                                          batch_size=16,\n",
    "                                                                          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
